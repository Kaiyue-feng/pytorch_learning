{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "用numpy实现两层神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    loss: 32419044.420235\n",
      "2    loss: 28803322.954488\n",
      "3    loss: 25447912.996801\n",
      "4    loss: 20299466.646728\n",
      "5    loss: 14334010.358585\n",
      "6    loss: 9100750.472797\n",
      "7    loss: 5533105.146304\n",
      "8    loss: 3406446.882498\n",
      "9    loss: 2223038.949741\n",
      "10    loss: 1558951.155826\n",
      "11    loss: 1168941.706120\n",
      "12    loss: 922449.728296\n",
      "13    loss: 753947.068483\n",
      "14    loss: 630579.139168\n",
      "15    loss: 535479.054370\n",
      "16    loss: 459587.778781\n",
      "17    loss: 397482.368722\n",
      "18    loss: 345769.850383\n",
      "19    loss: 302260.658377\n",
      "20    loss: 265383.392667\n",
      "21    loss: 233881.116057\n",
      "22    loss: 206809.473399\n",
      "23    loss: 183437.411048\n",
      "24    loss: 163175.493572\n",
      "25    loss: 145539.877877\n",
      "26    loss: 130142.055450\n",
      "27    loss: 116645.268470\n",
      "28    loss: 104781.081559\n",
      "29    loss: 94310.401773\n",
      "30    loss: 85048.498345\n",
      "31    loss: 76838.949862\n",
      "32    loss: 69545.902075\n",
      "33    loss: 63052.066269\n",
      "34    loss: 57255.775553\n",
      "35    loss: 52077.335237\n",
      "36    loss: 47436.875231\n",
      "37    loss: 43274.483364\n",
      "38    loss: 39529.531924\n",
      "39    loss: 36160.664927\n",
      "40    loss: 33119.982322\n",
      "41    loss: 30375.082927\n",
      "42    loss: 27896.776441\n",
      "43    loss: 25651.032720\n",
      "44    loss: 23614.752740\n",
      "45    loss: 21763.459399\n",
      "46    loss: 20079.762313\n",
      "47    loss: 18545.408869\n",
      "48    loss: 17145.557000\n",
      "49    loss: 15868.744194\n",
      "50    loss: 14701.724963\n",
      "51    loss: 13633.372965\n",
      "52    loss: 12654.376446\n",
      "53    loss: 11755.857746\n",
      "54    loss: 10930.831893\n",
      "55    loss: 10172.596173\n",
      "56    loss: 9474.786725\n",
      "57    loss: 8832.178555\n",
      "58    loss: 8239.802954\n",
      "59    loss: 7693.132525\n",
      "60    loss: 7188.062679\n",
      "61    loss: 6721.899317\n",
      "62    loss: 6290.618796\n",
      "63    loss: 5890.977523\n",
      "64    loss: 5520.613053\n",
      "65    loss: 5177.041638\n",
      "66    loss: 4858.006468\n",
      "67    loss: 4561.593947\n",
      "68    loss: 4286.110314\n",
      "69    loss: 4029.666813\n",
      "70    loss: 3790.829459\n",
      "71    loss: 3568.288086\n",
      "72    loss: 3360.743048\n",
      "73    loss: 3167.010409\n",
      "74    loss: 2986.076681\n",
      "75    loss: 2816.973497\n",
      "76    loss: 2658.900134\n",
      "77    loss: 2510.914321\n",
      "78    loss: 2372.357903\n",
      "79    loss: 2242.605348\n",
      "80    loss: 2120.926861\n",
      "81    loss: 2006.744989\n",
      "82    loss: 1899.582060\n",
      "83    loss: 1798.942988\n",
      "84    loss: 1704.435351\n",
      "85    loss: 1615.522835\n",
      "86    loss: 1531.883754\n",
      "87    loss: 1453.164502\n",
      "88    loss: 1379.059533\n",
      "89    loss: 1309.213279\n",
      "90    loss: 1243.362769\n",
      "91    loss: 1181.283276\n",
      "92    loss: 1122.718273\n",
      "93    loss: 1067.429211\n",
      "94    loss: 1015.200545\n",
      "95    loss: 965.844855\n",
      "96    loss: 919.192084\n",
      "97    loss: 875.088944\n",
      "98    loss: 833.349084\n",
      "99    loss: 793.840222\n",
      "100    loss: 756.430686\n",
      "101    loss: 721.004408\n",
      "102    loss: 687.433367\n",
      "103    loss: 655.614499\n",
      "104    loss: 625.446158\n",
      "105    loss: 596.813338\n",
      "106    loss: 569.642123\n",
      "107    loss: 543.845203\n",
      "108    loss: 519.354445\n",
      "109    loss: 496.073033\n",
      "110    loss: 473.952842\n",
      "111    loss: 452.925995\n",
      "112    loss: 432.954682\n",
      "113    loss: 413.945977\n",
      "114    loss: 395.858445\n",
      "115    loss: 378.641116\n",
      "116    loss: 362.251325\n",
      "117    loss: 346.643631\n",
      "118    loss: 331.766950\n",
      "119    loss: 317.589057\n",
      "120    loss: 304.080284\n",
      "121    loss: 291.196478\n",
      "122    loss: 278.905235\n",
      "123    loss: 267.179860\n",
      "124    loss: 255.991969\n",
      "125    loss: 245.313864\n",
      "126    loss: 235.115924\n",
      "127    loss: 225.377739\n",
      "128    loss: 216.079001\n",
      "129    loss: 207.193612\n",
      "130    loss: 198.707907\n",
      "131    loss: 190.591391\n",
      "132    loss: 182.832207\n",
      "133    loss: 175.414046\n",
      "134    loss: 168.320777\n",
      "135    loss: 161.534224\n",
      "136    loss: 155.042526\n",
      "137    loss: 148.829742\n",
      "138    loss: 142.883260\n",
      "139    loss: 137.192064\n",
      "140    loss: 131.742845\n",
      "141    loss: 126.523777\n",
      "142    loss: 121.525272\n",
      "143    loss: 116.737281\n",
      "144    loss: 112.149263\n",
      "145    loss: 107.753853\n",
      "146    loss: 103.540547\n",
      "147    loss: 99.502831\n",
      "148    loss: 95.632858\n",
      "149    loss: 91.920945\n",
      "150    loss: 88.361351\n",
      "151    loss: 84.948775\n",
      "152    loss: 81.674633\n",
      "153    loss: 78.533082\n",
      "154    loss: 75.519404\n",
      "155    loss: 72.627597\n",
      "156    loss: 69.852358\n",
      "157    loss: 67.188375\n",
      "158    loss: 64.630714\n",
      "159    loss: 62.175298\n",
      "160    loss: 59.817811\n",
      "161    loss: 57.554763\n",
      "162    loss: 55.381160\n",
      "163    loss: 53.293676\n",
      "164    loss: 51.288127\n",
      "165    loss: 49.361457\n",
      "166    loss: 47.510059\n",
      "167    loss: 45.731270\n",
      "168    loss: 44.021812\n",
      "169    loss: 42.379069\n",
      "170    loss: 40.799985\n",
      "171    loss: 39.282212\n",
      "172    loss: 37.823043\n",
      "173    loss: 36.420687\n",
      "174    loss: 35.072376\n",
      "175    loss: 33.775847\n",
      "176    loss: 32.528631\n",
      "177    loss: 31.329428\n",
      "178    loss: 30.175779\n",
      "179    loss: 29.066114\n",
      "180    loss: 27.998782\n",
      "181    loss: 26.972020\n",
      "182    loss: 25.984015\n",
      "183    loss: 25.033380\n",
      "184    loss: 24.118801\n",
      "185    loss: 23.238731\n",
      "186    loss: 22.391671\n",
      "187    loss: 21.576633\n",
      "188    loss: 20.792227\n",
      "189    loss: 20.036980\n",
      "190    loss: 19.309905\n",
      "191    loss: 18.610113\n",
      "192    loss: 17.936388\n",
      "193    loss: 17.287726\n",
      "194    loss: 16.663067\n",
      "195    loss: 16.061666\n",
      "196    loss: 15.482602\n",
      "197    loss: 14.924934\n",
      "198    loss: 14.387818\n",
      "199    loss: 13.870557\n",
      "200    loss: 13.372476\n",
      "201    loss: 12.892795\n",
      "202    loss: 12.430562\n",
      "203    loss: 11.985230\n",
      "204    loss: 11.556278\n",
      "205    loss: 11.143039\n",
      "206    loss: 10.745031\n",
      "207    loss: 10.361465\n",
      "208    loss: 9.991846\n",
      "209    loss: 9.635670\n",
      "210    loss: 9.292493\n",
      "211    loss: 8.961834\n",
      "212    loss: 8.643193\n",
      "213    loss: 8.336095\n",
      "214    loss: 8.040127\n",
      "215    loss: 7.754948\n",
      "216    loss: 7.480029\n",
      "217    loss: 7.214995\n",
      "218    loss: 6.959525\n",
      "219    loss: 6.713354\n",
      "220    loss: 6.476000\n",
      "221    loss: 6.247171\n",
      "222    loss: 6.026567\n",
      "223    loss: 5.813900\n",
      "224    loss: 5.608881\n",
      "225    loss: 5.411204\n",
      "226    loss: 5.220696\n",
      "227    loss: 5.036919\n",
      "228    loss: 4.859721\n",
      "229    loss: 4.688879\n",
      "230    loss: 4.524132\n",
      "231    loss: 4.365244\n",
      "232    loss: 4.212055\n",
      "233    loss: 4.064324\n",
      "234    loss: 3.921814\n",
      "235    loss: 3.784391\n",
      "236    loss: 3.651849\n",
      "237    loss: 3.524012\n",
      "238    loss: 3.400707\n",
      "239    loss: 3.281810\n",
      "240    loss: 3.167108\n",
      "241    loss: 3.056468\n",
      "242    loss: 2.949743\n",
      "243    loss: 2.846785\n",
      "244    loss: 2.747484\n",
      "245    loss: 2.651715\n",
      "246    loss: 2.559306\n",
      "247    loss: 2.470143\n",
      "248    loss: 2.384130\n",
      "249    loss: 2.301148\n",
      "250    loss: 2.221085\n",
      "251    loss: 2.143845\n",
      "252    loss: 2.069322\n",
      "253    loss: 1.997431\n",
      "254    loss: 1.928072\n",
      "255    loss: 1.861126\n",
      "256    loss: 1.796537\n",
      "257    loss: 1.734211\n",
      "258    loss: 1.674072\n",
      "259    loss: 1.616052\n",
      "260    loss: 1.560069\n",
      "261    loss: 1.506033\n",
      "262    loss: 1.453884\n",
      "263    loss: 1.403564\n",
      "264    loss: 1.354997\n",
      "265    loss: 1.308128\n",
      "266    loss: 1.262913\n",
      "267    loss: 1.219261\n",
      "268    loss: 1.177130\n",
      "269    loss: 1.136469\n",
      "270    loss: 1.097228\n",
      "271    loss: 1.059354\n",
      "272    loss: 1.022803\n",
      "273    loss: 0.987526\n",
      "274    loss: 0.953471\n",
      "275    loss: 0.920601\n",
      "276    loss: 0.888879\n",
      "277    loss: 0.858261\n",
      "278    loss: 0.828701\n",
      "279    loss: 0.800170\n",
      "280    loss: 0.772632\n",
      "281    loss: 0.746042\n",
      "282    loss: 0.720377\n",
      "283    loss: 0.695601\n",
      "284    loss: 0.671691\n",
      "285    loss: 0.648604\n",
      "286    loss: 0.626318\n",
      "287    loss: 0.604804\n",
      "288    loss: 0.584032\n",
      "289    loss: 0.563979\n",
      "290    loss: 0.544620\n",
      "291    loss: 0.525929\n",
      "292    loss: 0.507885\n",
      "293    loss: 0.490470\n",
      "294    loss: 0.473656\n",
      "295    loss: 0.457417\n",
      "296    loss: 0.441740\n",
      "297    loss: 0.426603\n",
      "298    loss: 0.411988\n",
      "299    loss: 0.397877\n",
      "300    loss: 0.384257\n",
      "301    loss: 0.371103\n",
      "302    loss: 0.358401\n",
      "303    loss: 0.346137\n",
      "304    loss: 0.334296\n",
      "305    loss: 0.322862\n",
      "306    loss: 0.311822\n",
      "307    loss: 0.301164\n",
      "308    loss: 0.290870\n",
      "309    loss: 0.280930\n",
      "310    loss: 0.271332\n",
      "311    loss: 0.262066\n",
      "312    loss: 0.253116\n",
      "313    loss: 0.244475\n",
      "314    loss: 0.236130\n",
      "315    loss: 0.228071\n",
      "316    loss: 0.220288\n",
      "317    loss: 0.212773\n",
      "318    loss: 0.205516\n",
      "319    loss: 0.198508\n",
      "320    loss: 0.191740\n",
      "321    loss: 0.185204\n",
      "322    loss: 0.178891\n",
      "323    loss: 0.172795\n",
      "324    loss: 0.166908\n",
      "325    loss: 0.161222\n",
      "326    loss: 0.155731\n",
      "327    loss: 0.150429\n",
      "328    loss: 0.145307\n",
      "329    loss: 0.140362\n",
      "330    loss: 0.135585\n",
      "331    loss: 0.130971\n",
      "332    loss: 0.126515\n",
      "333    loss: 0.122211\n",
      "334    loss: 0.118055\n",
      "335    loss: 0.114041\n",
      "336    loss: 0.110163\n",
      "337    loss: 0.106418\n",
      "338    loss: 0.102801\n",
      "339    loss: 0.099307\n",
      "340    loss: 0.095933\n",
      "341    loss: 0.092675\n",
      "342    loss: 0.089527\n",
      "343    loss: 0.086486\n",
      "344    loss: 0.083549\n",
      "345    loss: 0.080713\n",
      "346    loss: 0.077973\n",
      "347    loss: 0.075326\n",
      "348    loss: 0.072771\n",
      "349    loss: 0.070302\n",
      "350    loss: 0.067917\n",
      "351    loss: 0.065613\n",
      "352    loss: 0.063387\n",
      "353    loss: 0.061238\n",
      "354    loss: 0.059162\n",
      "355    loss: 0.057156\n",
      "356    loss: 0.055219\n",
      "357    loss: 0.053347\n",
      "358    loss: 0.051539\n",
      "359    loss: 0.049793\n",
      "360    loss: 0.048106\n",
      "361    loss: 0.046476\n",
      "362    loss: 0.044903\n",
      "363    loss: 0.043382\n",
      "364    loss: 0.041913\n",
      "365    loss: 0.040494\n",
      "366    loss: 0.039123\n",
      "367    loss: 0.037799\n",
      "368    loss: 0.036520\n",
      "369    loss: 0.035284\n",
      "370    loss: 0.034090\n",
      "371    loss: 0.032937\n",
      "372    loss: 0.031823\n",
      "373    loss: 0.030746\n",
      "374    loss: 0.029706\n",
      "375    loss: 0.028702\n",
      "376    loss: 0.027732\n",
      "377    loss: 0.026794\n",
      "378    loss: 0.025888\n",
      "379    loss: 0.025013\n",
      "380    loss: 0.024168\n",
      "381    loss: 0.023351\n",
      "382    loss: 0.022562\n",
      "383    loss: 0.021800\n",
      "384    loss: 0.021064\n",
      "385    loss: 0.020352\n",
      "386    loss: 0.019665\n",
      "387    loss: 0.019001\n",
      "388    loss: 0.018359\n",
      "389    loss: 0.017740\n",
      "390    loss: 0.017141\n",
      "391    loss: 0.016562\n",
      "392    loss: 0.016003\n",
      "393    loss: 0.015463\n",
      "394    loss: 0.014941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395    loss: 0.014437\n",
      "396    loss: 0.013950\n",
      "397    loss: 0.013480\n",
      "398    loss: 0.013025\n",
      "399    loss: 0.012586\n",
      "400    loss: 0.012161\n",
      "401    loss: 0.011751\n",
      "402    loss: 0.011355\n",
      "403    loss: 0.010972\n",
      "404    loss: 0.010602\n",
      "405    loss: 0.010245\n",
      "406    loss: 0.009900\n",
      "407    loss: 0.009566\n",
      "408    loss: 0.009244\n",
      "409    loss: 0.008933\n",
      "410    loss: 0.008632\n",
      "411    loss: 0.008341\n",
      "412    loss: 0.008060\n",
      "413    loss: 0.007788\n",
      "414    loss: 0.007526\n",
      "415    loss: 0.007273\n",
      "416    loss: 0.007028\n",
      "417    loss: 0.006791\n",
      "418    loss: 0.006563\n",
      "419    loss: 0.006342\n",
      "420    loss: 0.006128\n",
      "421    loss: 0.005922\n",
      "422    loss: 0.005723\n",
      "423    loss: 0.005530\n",
      "424    loss: 0.005344\n",
      "425    loss: 0.005164\n",
      "426    loss: 0.004991\n",
      "427    loss: 0.004823\n",
      "428    loss: 0.004661\n",
      "429    loss: 0.004504\n",
      "430    loss: 0.004352\n",
      "431    loss: 0.004206\n",
      "432    loss: 0.004065\n",
      "433    loss: 0.003928\n",
      "434    loss: 0.003796\n",
      "435    loss: 0.003668\n",
      "436    loss: 0.003545\n",
      "437    loss: 0.003426\n",
      "438    loss: 0.003311\n",
      "439    loss: 0.003199\n",
      "440    loss: 0.003092\n",
      "441    loss: 0.002988\n",
      "442    loss: 0.002888\n",
      "443    loss: 0.002791\n",
      "444    loss: 0.002697\n",
      "445    loss: 0.002606\n",
      "446    loss: 0.002519\n",
      "447    loss: 0.002434\n",
      "448    loss: 0.002352\n",
      "449    loss: 0.002273\n",
      "450    loss: 0.002197\n",
      "451    loss: 0.002123\n",
      "452    loss: 0.002052\n",
      "453    loss: 0.001983\n",
      "454    loss: 0.001917\n",
      "455    loss: 0.001852\n",
      "456    loss: 0.001790\n",
      "457    loss: 0.001730\n",
      "458    loss: 0.001672\n",
      "459    loss: 0.001616\n",
      "460    loss: 0.001562\n",
      "461    loss: 0.001509\n",
      "462    loss: 0.001459\n",
      "463    loss: 0.001410\n",
      "464    loss: 0.001362\n",
      "465    loss: 0.001317\n",
      "466    loss: 0.001273\n",
      "467    loss: 0.001230\n",
      "468    loss: 0.001189\n",
      "469    loss: 0.001149\n",
      "470    loss: 0.001110\n",
      "471    loss: 0.001073\n",
      "472    loss: 0.001037\n",
      "473    loss: 0.001002\n",
      "474    loss: 0.000969\n",
      "475    loss: 0.000936\n",
      "476    loss: 0.000905\n",
      "477    loss: 0.000875\n",
      "478    loss: 0.000845\n",
      "479    loss: 0.000817\n",
      "480    loss: 0.000790\n",
      "481    loss: 0.000763\n",
      "482    loss: 0.000738\n",
      "483    loss: 0.000713\n",
      "484    loss: 0.000689\n",
      "485    loss: 0.000666\n",
      "486    loss: 0.000644\n",
      "487    loss: 0.000622\n",
      "488    loss: 0.000601\n",
      "489    loss: 0.000581\n",
      "490    loss: 0.000562\n",
      "491    loss: 0.000543\n",
      "492    loss: 0.000525\n",
      "493    loss: 0.000507\n",
      "494    loss: 0.000490\n",
      "495    loss: 0.000474\n",
      "496    loss: 0.000458\n",
      "497    loss: 0.000442\n",
      "498    loss: 0.000428\n",
      "499    loss: 0.000413\n",
      "500    loss: 0.000400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64,1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(500):\n",
    "    h = x.dot(w1)\n",
    "    a = np.maximum(h, 0)\n",
    "#     print(a.shape)\n",
    "    y_pred = a.dot(w2)\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    \n",
    "    grad_y_pred = 2*(y_pred - y)\n",
    "#     print(grad_y_pred.shape)\n",
    "    grad_w2 = a.T.dot(grad_y_pred)\n",
    "#     print('gw2',grad_w2.shape)\n",
    "    grad_a = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_a.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "#     print('gw1', grad_w1.shape)\n",
    "    \n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2\n",
    "    print(i+1,'   loss: %f'%loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors\n",
    "----------------\n",
    "\n",
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N * H\n",
    "    h_relu = h.clamp(min=0) # N * H\n",
    "    y_pred = h_relu.mm(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensor和autograd\n",
    "-------------------------------\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: optim\n",
    "--------------\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "x = torch.randn(64, 1000)\n",
    "y = torch.randn(64, 10)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1000, 100, bias = False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10, bias = False)\n",
    ")\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "for it in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(it+1,loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: 自定义 nn Modules\n",
    "--------------------------\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 708.2902221679688\n",
      "1 690.6024780273438\n",
      "2 673.4022216796875\n",
      "3 656.6740112304688\n",
      "4 640.3989868164062\n",
      "5 624.6063842773438\n",
      "6 609.2277221679688\n",
      "7 594.2924194335938\n",
      "8 579.7576293945312\n",
      "9 565.6204833984375\n",
      "10 551.8333129882812\n",
      "11 538.4389038085938\n",
      "12 525.4700927734375\n",
      "13 512.8477783203125\n",
      "14 500.6307373046875\n",
      "15 488.703369140625\n",
      "16 477.16925048828125\n",
      "17 465.9186096191406\n",
      "18 454.9781799316406\n",
      "19 444.3111877441406\n",
      "20 433.9541015625\n",
      "21 423.9173889160156\n",
      "22 414.1399841308594\n",
      "23 404.56536865234375\n",
      "24 395.1788024902344\n",
      "25 386.0086975097656\n",
      "26 377.04425048828125\n",
      "27 368.3394470214844\n",
      "28 359.90484619140625\n",
      "29 351.6649169921875\n",
      "30 343.64385986328125\n",
      "31 335.83502197265625\n",
      "32 328.1756591796875\n",
      "33 320.7092590332031\n",
      "34 313.4267272949219\n",
      "35 306.3157958984375\n",
      "36 299.377685546875\n",
      "37 292.6019287109375\n",
      "38 285.9645690917969\n",
      "39 279.4676208496094\n",
      "40 273.1219787597656\n",
      "41 266.9126281738281\n",
      "42 260.8498840332031\n",
      "43 254.896240234375\n",
      "44 249.0304412841797\n",
      "45 243.2618865966797\n",
      "46 237.6168212890625\n",
      "47 232.09451293945312\n",
      "48 226.66732788085938\n",
      "49 221.33404541015625\n",
      "50 216.1132049560547\n",
      "51 211.00015258789062\n",
      "52 205.9881591796875\n",
      "53 201.0690155029297\n",
      "54 196.24990844726562\n",
      "55 191.53265380859375\n",
      "56 186.9126434326172\n",
      "57 182.39088439941406\n",
      "58 177.96060180664062\n",
      "59 173.61810302734375\n",
      "60 169.3471221923828\n",
      "61 165.1629180908203\n",
      "62 161.0652618408203\n",
      "63 157.05921936035156\n",
      "64 153.1293182373047\n",
      "65 149.2676544189453\n",
      "66 145.4900665283203\n",
      "67 141.79226684570312\n",
      "68 138.17279052734375\n",
      "69 134.64390563964844\n",
      "70 131.19601440429688\n",
      "71 127.8241958618164\n",
      "72 124.51718139648438\n",
      "73 121.27862548828125\n",
      "74 118.1079330444336\n",
      "75 115.00167846679688\n",
      "76 111.95262145996094\n",
      "77 108.97331237792969\n",
      "78 106.0565414428711\n",
      "79 103.19403076171875\n",
      "80 100.39382934570312\n",
      "81 97.6460189819336\n",
      "82 94.95909118652344\n",
      "83 92.33007049560547\n",
      "84 89.75663757324219\n",
      "85 87.23802185058594\n",
      "86 84.77288055419922\n",
      "87 82.3656997680664\n",
      "88 80.0083999633789\n",
      "89 77.70929718017578\n",
      "90 75.46202087402344\n",
      "91 73.26372528076172\n",
      "92 71.11355590820312\n",
      "93 69.01181030273438\n",
      "94 66.96241760253906\n",
      "95 64.96072387695312\n",
      "96 63.008453369140625\n",
      "97 61.10183334350586\n",
      "98 59.24684143066406\n",
      "99 57.437965393066406\n",
      "100 55.672035217285156\n",
      "101 53.95510482788086\n",
      "102 52.280555725097656\n",
      "103 50.64847183227539\n",
      "104 49.05857467651367\n",
      "105 47.505462646484375\n",
      "106 45.996612548828125\n",
      "107 44.528099060058594\n",
      "108 43.099239349365234\n",
      "109 41.71031188964844\n",
      "110 40.36016082763672\n",
      "111 39.04526138305664\n",
      "112 37.7664680480957\n",
      "113 36.52466583251953\n",
      "114 35.31774139404297\n",
      "115 34.14537048339844\n",
      "116 33.00534439086914\n",
      "117 31.898439407348633\n",
      "118 30.82370948791504\n",
      "119 29.780807495117188\n",
      "120 28.766889572143555\n",
      "121 27.7854061126709\n",
      "122 26.83487319946289\n",
      "123 25.910228729248047\n",
      "124 25.014198303222656\n",
      "125 24.146526336669922\n",
      "126 23.30541229248047\n",
      "127 22.489627838134766\n",
      "128 21.69859504699707\n",
      "129 20.931346893310547\n",
      "130 20.188308715820312\n",
      "131 19.468669891357422\n",
      "132 18.770526885986328\n",
      "133 18.095781326293945\n",
      "134 17.44333267211914\n",
      "135 16.811063766479492\n",
      "136 16.19881248474121\n",
      "137 15.606346130371094\n",
      "138 15.034292221069336\n",
      "139 14.480680465698242\n",
      "140 13.945107460021973\n",
      "141 13.428689956665039\n",
      "142 12.929701805114746\n",
      "143 12.447736740112305\n",
      "144 11.982421875\n",
      "145 11.533933639526367\n",
      "146 11.100412368774414\n",
      "147 10.68177604675293\n",
      "148 10.27778148651123\n",
      "149 9.888238906860352\n",
      "150 9.512951850891113\n",
      "151 9.151009559631348\n",
      "152 8.801077842712402\n",
      "153 8.463823318481445\n",
      "154 8.138301849365234\n",
      "155 7.82467794418335\n",
      "156 7.522037506103516\n",
      "157 7.229952812194824\n",
      "158 6.948527812957764\n",
      "159 6.677139759063721\n",
      "160 6.416045188903809\n",
      "161 6.163891315460205\n",
      "162 5.921058177947998\n",
      "163 5.68699312210083\n",
      "164 5.461601257324219\n",
      "165 5.244247913360596\n",
      "166 5.035066604614258\n",
      "167 4.833858489990234\n",
      "168 4.640171051025391\n",
      "169 4.4537353515625\n",
      "170 4.274376392364502\n",
      "171 4.10187292098999\n",
      "172 3.935567855834961\n",
      "173 3.7757480144500732\n",
      "174 3.622141122817993\n",
      "175 3.4743804931640625\n",
      "176 3.332371473312378\n",
      "177 3.19571852684021\n",
      "178 3.064422845840454\n",
      "179 2.938436508178711\n",
      "180 2.8171396255493164\n",
      "181 2.7008352279663086\n",
      "182 2.5888373851776123\n",
      "183 2.481262683868408\n",
      "184 2.3780016899108887\n",
      "185 2.278665781021118\n",
      "186 2.183295249938965\n",
      "187 2.0916080474853516\n",
      "188 2.0035240650177\n",
      "189 1.9189741611480713\n",
      "190 1.837730884552002\n",
      "191 1.7597180604934692\n",
      "192 1.6847912073135376\n",
      "193 1.6128859519958496\n",
      "194 1.543887972831726\n",
      "195 1.4776997566223145\n",
      "196 1.414184808731079\n",
      "197 1.3532919883728027\n",
      "198 1.2948819398880005\n",
      "199 1.2389211654663086\n",
      "200 1.185302734375\n",
      "201 1.1339412927627563\n",
      "202 1.0848748683929443\n",
      "203 1.0379303693771362\n",
      "204 0.9929629564285278\n",
      "205 0.9499207735061646\n",
      "206 0.9087198972702026\n",
      "207 0.8692552447319031\n",
      "208 0.831474781036377\n",
      "209 0.795340359210968\n",
      "210 0.7607523202896118\n",
      "211 0.7276688814163208\n",
      "212 0.6960501074790955\n",
      "213 0.6657907962799072\n",
      "214 0.6368427872657776\n",
      "215 0.6091143488883972\n",
      "216 0.5826082229614258\n",
      "217 0.5572586059570312\n",
      "218 0.5329903960227966\n",
      "219 0.5097693204879761\n",
      "220 0.48756903409957886\n",
      "221 0.4663442075252533\n",
      "222 0.44602102041244507\n",
      "223 0.42658519744873047\n",
      "224 0.4080018699169159\n",
      "225 0.39023980498313904\n",
      "226 0.37324514985084534\n",
      "227 0.3569869101047516\n",
      "228 0.3414473533630371\n",
      "229 0.32658061385154724\n",
      "230 0.31236717104911804\n",
      "231 0.2987830936908722\n",
      "232 0.2857995331287384\n",
      "233 0.2734600305557251\n",
      "234 0.26166626811027527\n",
      "235 0.25039124488830566\n",
      "236 0.23962512612342834\n",
      "237 0.22931981086730957\n",
      "238 0.21947328746318817\n",
      "239 0.2100575715303421\n",
      "240 0.2010570764541626\n",
      "241 0.1924457997083664\n",
      "242 0.18422286212444305\n",
      "243 0.17634756863117218\n",
      "244 0.1688249111175537\n",
      "245 0.16162866353988647\n",
      "246 0.1547558456659317\n",
      "247 0.1481722593307495\n",
      "248 0.1418793499469757\n",
      "249 0.135865718126297\n",
      "250 0.1301087588071823\n",
      "251 0.12460720539093018\n",
      "252 0.11934769153594971\n",
      "253 0.1143292784690857\n",
      "254 0.10951551795005798\n",
      "255 0.10491861402988434\n",
      "256 0.10052221268415451\n",
      "257 0.09631114453077316\n",
      "258 0.09228523820638657\n",
      "259 0.0884336307644844\n",
      "260 0.08474872261285782\n",
      "261 0.08122207224369049\n",
      "262 0.07784762978553772\n",
      "263 0.07461840659379959\n",
      "264 0.07152719050645828\n",
      "265 0.06856997311115265\n",
      "266 0.06573866307735443\n",
      "267 0.06303033232688904\n",
      "268 0.06043538078665733\n",
      "269 0.05795241892337799\n",
      "270 0.055575210601091385\n",
      "271 0.05329900234937668\n",
      "272 0.0511193722486496\n",
      "273 0.04903369024395943\n",
      "274 0.04703535884618759\n",
      "275 0.04512197896838188\n",
      "276 0.043290071189403534\n",
      "277 0.04153876006603241\n",
      "278 0.03986380621790886\n",
      "279 0.03825587406754494\n",
      "280 0.03671673312783241\n",
      "281 0.03524263575673103\n",
      "282 0.03382929414510727\n",
      "283 0.0324748270213604\n",
      "284 0.03117726743221283\n",
      "285 0.029933372512459755\n",
      "286 0.028741460293531418\n",
      "287 0.02759825624525547\n",
      "288 0.026502706110477448\n",
      "289 0.02545137330889702\n",
      "290 0.024443810805678368\n",
      "291 0.023477431386709213\n",
      "292 0.022550545632839203\n",
      "293 0.021661242470145226\n",
      "294 0.020808542147278786\n",
      "295 0.019990433007478714\n",
      "296 0.019205641001462936\n",
      "297 0.01845320500433445\n",
      "298 0.017729956656694412\n",
      "299 0.017036335542798042\n",
      "300 0.01637132093310356\n",
      "301 0.015732232481241226\n",
      "302 0.015118995681405067\n",
      "303 0.014530284330248833\n",
      "304 0.01396528072655201\n",
      "305 0.013422559015452862\n",
      "306 0.012901470065116882\n",
      "307 0.012401068583130836\n",
      "308 0.011920513585209846\n",
      "309 0.01145905815064907\n",
      "310 0.01101608481258154\n",
      "311 0.010590328834950924\n",
      "312 0.010181131772696972\n",
      "313 0.009788296185433865\n",
      "314 0.009411290287971497\n",
      "315 0.009048184379935265\n",
      "316 0.008699566125869751\n",
      "317 0.008364865556359291\n",
      "318 0.00804293155670166\n",
      "319 0.00773376040160656\n",
      "320 0.0074363285675644875\n",
      "321 0.007150559686124325\n",
      "322 0.006875861901789904\n",
      "323 0.006611788645386696\n",
      "324 0.006357904057949781\n",
      "325 0.006113843992352486\n",
      "326 0.005879221949726343\n",
      "327 0.005653537809848785\n",
      "328 0.005436587147414684\n",
      "329 0.005227976944297552\n",
      "330 0.00502735422924161\n",
      "331 0.0048344130627810955\n",
      "332 0.004648934584110975\n",
      "333 0.00447052251547575\n",
      "334 0.004298903979361057\n",
      "335 0.004133834969252348\n",
      "336 0.003975131083279848\n",
      "337 0.0038224365562200546\n",
      "338 0.003675575600937009\n",
      "339 0.003534367773681879\n",
      "340 0.0033985008485615253\n",
      "341 0.0032677920535206795\n",
      "342 0.0031421170569956303\n",
      "343 0.0030211948323994875\n",
      "344 0.0029048656579107046\n",
      "345 0.0027929889038205147\n",
      "346 0.0026853575836867094\n",
      "347 0.0025818876456469297\n",
      "348 0.0024822154082357883\n",
      "349 0.0023864293470978737\n",
      "350 0.0022942442446947098\n",
      "351 0.002205586526542902\n",
      "352 0.002120302990078926\n",
      "353 0.0020382851362228394\n",
      "354 0.0019594563636928797\n",
      "355 0.0018834798829630017\n",
      "356 0.001810445450246334\n",
      "357 0.0017402364173904061\n",
      "358 0.0016726820031180978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 0.001607690704986453\n",
      "360 0.001545189879834652\n",
      "361 0.0014850537991151214\n",
      "362 0.001427192590199411\n",
      "363 0.001371570280753076\n",
      "364 0.0013180422829464078\n",
      "365 0.0012665791437029839\n",
      "366 0.0012170739937573671\n",
      "367 0.0011694631539285183\n",
      "368 0.0011236702557653189\n",
      "369 0.0010796217247843742\n",
      "370 0.001037268782965839\n",
      "371 0.000996532035060227\n",
      "372 0.0009573434945195913\n",
      "373 0.0009196847677230835\n",
      "374 0.0008834534673951566\n",
      "375 0.0008486151928082108\n",
      "376 0.000815116218291223\n",
      "377 0.0007829057285562158\n",
      "378 0.0007519570644944906\n",
      "379 0.000722153636161238\n",
      "380 0.000693541660439223\n",
      "381 0.000666032254230231\n",
      "382 0.0006395739037543535\n",
      "383 0.0006141540943644941\n",
      "384 0.0005896971561014652\n",
      "385 0.0005662099574692547\n",
      "386 0.0005436079809442163\n",
      "387 0.0005218963488005102\n",
      "388 0.0005010328604839742\n",
      "389 0.00048097403487190604\n",
      "390 0.00046169120469130576\n",
      "391 0.0004431648994795978\n",
      "392 0.0004253685474395752\n",
      "393 0.0004082614032085985\n",
      "394 0.0003918131405953318\n",
      "395 0.0003760193649213761\n",
      "396 0.00036083365557715297\n",
      "397 0.0003462542372290045\n",
      "398 0.00033224659273400903\n",
      "399 0.00031878624577075243\n",
      "400 0.0003058604779653251\n",
      "401 0.0002934350341092795\n",
      "402 0.0002815012994688004\n",
      "403 0.0002700435870792717\n",
      "404 0.00025903902132995427\n",
      "405 0.0002484660944901407\n",
      "406 0.00023831271391827613\n",
      "407 0.000228560296818614\n",
      "408 0.00021919995197094977\n",
      "409 0.00021021444990765303\n",
      "410 0.0002015776262851432\n",
      "411 0.0001932924205902964\n",
      "412 0.00018533224647399038\n",
      "413 0.0001776976278051734\n",
      "414 0.00017036018834915012\n",
      "415 0.00016332110681105405\n",
      "416 0.00015656686446163803\n",
      "417 0.00015007535694167018\n",
      "418 0.00014384988753590733\n",
      "419 0.00013787552597932518\n",
      "420 0.00013214288628660142\n",
      "421 0.00012663590314332396\n",
      "422 0.00012135814904468134\n",
      "423 0.00011629760410869494\n",
      "424 0.0001114314582082443\n",
      "425 0.00010676965757738799\n",
      "426 0.00010229320469079539\n",
      "427 9.80027107289061e-05\n",
      "428 9.388831676915288e-05\n",
      "429 8.993490337161347e-05\n",
      "430 8.615014667157084e-05\n",
      "431 8.25158494990319e-05\n",
      "432 7.902755896793678e-05\n",
      "433 7.568812725367025e-05\n",
      "434 7.248466863529757e-05\n",
      "435 6.94072077749297e-05\n",
      "436 6.64635153952986e-05\n",
      "437 6.363992724800482e-05\n",
      "438 6.0928872699150816e-05\n",
      "439 5.833313116454519e-05\n",
      "440 5.584296741290018e-05\n",
      "441 5.3454965382115915e-05\n",
      "442 5.116898682899773e-05\n",
      "443 4.8976289690472186e-05\n",
      "444 4.687366163125262e-05\n",
      "445 4.486049510887824e-05\n",
      "446 4.293285746825859e-05\n",
      "447 4.108054417883977e-05\n",
      "448 3.931108585675247e-05\n",
      "449 3.761139669222757e-05\n",
      "450 3.5984270652988926e-05\n",
      "451 3.4424399927956983e-05\n",
      "452 3.293287591077387e-05\n",
      "453 3.1498777389060706e-05\n",
      "454 3.013168679899536e-05\n",
      "455 2.8819886210840195e-05\n",
      "456 2.756359026534483e-05\n",
      "457 2.635905912029557e-05\n",
      "458 2.5206356440321542e-05\n",
      "459 2.4101716917357408e-05\n",
      "460 2.3048140064929612e-05\n",
      "461 2.2033877030480653e-05\n",
      "462 2.1064317479613237e-05\n",
      "463 2.013828998315148e-05\n",
      "464 1.925289507198613e-05\n",
      "465 1.8400942281004973e-05\n",
      "466 1.75884306372609e-05\n",
      "467 1.680991226749029e-05\n",
      "468 1.606548175914213e-05\n",
      "469 1.5352377886301838e-05\n",
      "470 1.4669114534626715e-05\n",
      "471 1.401747613272164e-05\n",
      "472 1.339224854746135e-05\n",
      "473 1.2793801943189465e-05\n",
      "474 1.2221898941788822e-05\n",
      "475 1.167622485809261e-05\n",
      "476 1.1152385923196562e-05\n",
      "477 1.065170181391295e-05\n",
      "478 1.0173528607992921e-05\n",
      "479 9.715447959024459e-06\n",
      "480 9.277492608816829e-06\n",
      "481 8.858584806148428e-06\n",
      "482 8.459163836960215e-06\n",
      "483 8.07605010777479e-06\n",
      "484 7.709140845690854e-06\n",
      "485 7.3593737397459336e-06\n",
      "486 7.024824753898429e-06\n",
      "487 6.706376098009059e-06\n",
      "488 6.4006608226918615e-06\n",
      "489 6.108665729698259e-06\n",
      "490 5.828618668601848e-06\n",
      "491 5.562427304539597e-06\n",
      "492 5.307648279995192e-06\n",
      "493 5.064454398961971e-06\n",
      "494 4.830748821404995e-06\n",
      "495 4.6090299292700365e-06\n",
      "496 4.396055828692624e-06\n",
      "497 4.193889708403731e-06\n",
      "498 4.000286480732029e-06\n",
      "499 3.814937372226268e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
